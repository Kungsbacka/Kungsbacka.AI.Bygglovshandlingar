{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c14819-25c8-42f6-9653-5275545ef908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e705a123-3c7f-4da3-8c36-e1b8fc7ca94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0dbeb-407f-462b-adc2-b5cd3f211bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ae7e4-080a-4fe7-bf9e-e5b72098ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test[\"annotation_type\"] == \"Rectangle\"][ df_test[\"label\"] == \"riktning_symbol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b9dba-0205-4607-a222-2105e6fee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed720b9-3cc0-4967-85b2-2fab18c42f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader for the dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class KBABygglovDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, scale = 4):\n",
    "        self.df = df\n",
    "        self.transforms = None\n",
    "        self.scale = scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = cv2.imread(row[\"image\"])\n",
    "        #contour = find_bounding_box(image)\n",
    "        \n",
    "        \n",
    "        # Normalize bounding box coordinates\n",
    "        width, height = row[\"original_width\"], row[\"original_height\"]\n",
    "        target = {}\n",
    "\n",
    "        x1 = row['x'] * width //100 \n",
    "        y1 = row['y'] * height //100\n",
    "        width = row['width'] * width //100\n",
    "        height = row['height'] * height //100\n",
    "        target[\"boxes\"] = torch.as_tensor([[x1,y1, x1 + width, y1 + height]], \n",
    "                                dtype = torch.float32)\n",
    "        target[\"labels\"]=torch.as_tensor([1],\n",
    "                        dtype = torch.int64)\n",
    "        target[\"image_id\"] = torch.as_tensor(idx)\n",
    "        #scale the image and target to half size\n",
    "        image = cv2.resize(image, (image.shape[1]// self.scale,  image.shape[0]// self.scale))\n",
    "\n",
    "        target[\"boxes\"] = target[\"boxes\"]/ self.scale\n",
    "\n",
    "        image = F.to_tensor(image)\n",
    "        #cropped_image, target = crop_to_contour(image, target, contour)\n",
    "        return image, target\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = KBABygglovDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06dd434-56a1-4944-a81a-899368be115c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collate image-target pairs into a tuple.\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "# Create the DataLoaders from the Datasets. \n",
    "\n",
    "'''val_dl = torch.utils.data.DataLoader(val_ds, \n",
    "                             batch_size = 4, \n",
    "                            shuffle = False, \n",
    "                    collate_fn = collate_fn)'''\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, \n",
    "                               batch_size = 1, \n",
    "                              shuffle = False, \n",
    "                      collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516d1652-537d-488e-8665-14b557ca5728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3d10a1-fa23-475d-9e03-7df95eea6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('riktning.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9a129d-27db-4eaa-aa8f-c2f90a17e3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unbatch(batch, device):\n",
    "    \"\"\"\n",
    "    Unbatches a batch of data from the Dataloader.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        X: list\n",
    "            List of images.\n",
    "        y: list\n",
    "            List of dictionaries.\n",
    "    \"\"\"\n",
    "    X, y = batch\n",
    "    X = [x.to(device) for x in X]\n",
    "    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "    return X, y\n",
    "def train_batch(batch, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Uses back propagation to train a model.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        model: torch model\n",
    "        optimizer: torch optimizer\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        loss: float\n",
    "            Sum of the batch losses.\n",
    "        losses: dict\n",
    "            Dictionary containing the individual losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    X, y = unbatch(batch, device = device)\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(X, y)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "@torch.no_grad()\n",
    "def validate_batch(batch, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Evaluates a model's loss value using validation data.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        model: torch model\n",
    "        optimizer: torch optimizer\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        loss: float\n",
    "            Sum of the batch losses.\n",
    "        losses: dict\n",
    "            Dictionary containing the individual losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    X, y = unbatch(batch, device = device)\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(X, y)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1645dc44-f05a-4842-9716-0f233acae1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_batch(batch, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X, _ = unbatch(batch, device = device)\n",
    "    predictions = model(X)\n",
    "    return [x.cpu() for x in X], predictions\n",
    "def predict(model, data_loader, device = \"cpu\"):\n",
    "    images = []\n",
    "    predictions = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        X, p = predict_batch(batch, model, device)\n",
    "        images.append(X)\n",
    "        predictions.append(p)\n",
    "    \n",
    "    return images, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc61c36-382d-49ca-8b11-7f8f47c0649d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "images, predictions = predict(model, test_dl, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80827f6c-238e-42da-a178-8030823d4a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac7c86-6d9e-428a-a12d-0e31d05b6c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83597b32-6612-42b7-a9a9-5267456620e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def decode_prediction(prediction, \n",
    "                      score_threshold = 0.8, \n",
    "                      nms_iou_threshold = 0.2):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        prediction: dict\n",
    "        score_threshold: float\n",
    "        nms_iou_threshold: float\n",
    "    Returns\n",
    "        prediction: tuple\n",
    "    \"\"\"\n",
    "    boxes = prediction[\"boxes\"]\n",
    "    scores = prediction[\"scores\"]\n",
    "    labels = prediction[\"labels\"]\n",
    "    # Remove any low-score predictions.\n",
    "    if score_threshold is not None:\n",
    "        want = scores > score_threshold\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]\n",
    "    # Remove any overlapping bounding boxes using NMS.\n",
    "    if nms_iou_threshold is not None:\n",
    "        want = torchvision.ops.nms(boxes = boxes, scores = scores, \n",
    "                                iou_threshold = nms_iou_threshold)\n",
    "        boxes = boxes[want]\n",
    "        scores = scores[want]\n",
    "        labels = labels[want]\n",
    "    return {\"boxes\": boxes, \n",
    "            \"labels\": labels, \n",
    "            \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab89cdd-2bef-4b91-bdbd-7adc2e705ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Object_detection.object_detection_helper import visualize_image_with_bounding_boxes_colored_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a554f-a086-4318-b2f0-cdbab1f80f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(images)):\n",
    "    print(i)\n",
    "    new_pred = decode_prediction(predictions[i][0], score_threshold = 0.5)\n",
    "    print(new_pred)\n",
    "    visualize_image_with_bounding_boxes_colored_img(images[i][0], new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cde68-7d0b-441b-98ff-f87b58d47061",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_examples = pd.read_csv(\"annotations.csv\")\n",
    "negative_examples = negative_examples[negative_examples[\"annotation_type\"] == \"Rectangle\"][ negative_examples[\"label\"] == \"riktning_saknas\"]\n",
    "negative_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be075e6b-cda7-4599-a770-639bf67580d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader for the dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "\n",
    "class KBABygglovDataset_false(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.transforms = None\n",
    "        print(len(self.df))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = self.df.iloc[idx][\"image\"]\n",
    "        image = Image.open(image_path)\n",
    "        image = F.to_tensor(image)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"id\"] = torch.as_tensor(row[\"annotation_id\"])\n",
    "        \n",
    "        return image, target\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1323af-cffb-4bba-9653-dd9fc49a2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_false = KBABygglovDataset_false(negative_examples)\n",
    "false_dl = torch.utils.data.DataLoader(dataset_false, \n",
    "                                 batch_size =1, \n",
    "                                 shuffle = False, \n",
    "                        collate_fn = collate_fn)\n",
    "images, predictions = predict(model, false_dl, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebecacc-3ce0-4d2a-a26b-86af9b334bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(images)):\n",
    "    print(i)\n",
    "    new_pred = decode_prediction(predictions[i][0], score_threshold = 0.5)\n",
    "    print(new_pred)\n",
    "    visualize_image_with_bounding_boxes_colored_img(images[i][0], new_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69c03f-195d-4a09-9f53-995cf8542774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac765cd-38a4-4c76-ab96-08fd51eb6c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m121"
  },
  "kernelspec": {
   "display_name": "kba-bygglov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
