{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01213688-04ea-4f82-a5b1-2c9ae69a91f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9ea1ba-9ada-42ff-a13a-2f0a832422f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95d73c-6648-4d98-a908-87a6bcc0a2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6dd9cd-a0b4-4e16-9b93-a102a0e91836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df[\"annotation_type\"] == \"KeyPoint\"][ df[\"label\"] == \"ny_marklinje\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51538f-8201-48de-bd76-87b725b519ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd15464b-d87a-4a7a-8466-00dafae6de5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "def visualize_image_with_bounding_boxes_colored(df):\n",
    "    \"\"\"\n",
    "    Visualizes an image with bounding boxes, each colored according to the rectanglelabels.\n",
    "    Adds labels and colors to the legend.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the image file.\n",
    "    - bounding_boxes: A list of bounding boxes, where each bounding box is represented as a dictionary\n",
    "      with keys 'x', 'y', 'width', 'height', and optionally 'label'. Coordinates are normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    # Open the image file\n",
    "    img = Image.open(df[\"image\"])\n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.set_size_inches(10, 10)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Image dimensions\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Label to color mapping\n",
    "    label_color_map = {\n",
    "        'riktning_text': 'r',\n",
    "        'another_label': 'g',\n",
    "          # Example additional label\n",
    "        # Add more labels and colors as needed\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Denormalize coordinates\n",
    "    x = df['x'] * img_width //100\n",
    "    y = df['y'] * img_height //100\n",
    "    \n",
    "    # Get color for the label\n",
    "    label = df['label']\n",
    "    color = label_color_map.get(label, 'b')  # Default to blue if label not in map\n",
    "    \n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x, y), 2, 2, linewidth=2, edgecolor=color, facecolor=color, label=label)\n",
    "    \n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Create legend from unique labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))  # Removing duplicates\n",
    "    ax.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21116ddd-a3cb-402c-9bd8-3769fa50367f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b2ace-e60f-40a5-8f54-8b9f1c46cfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawings = df.groupby('task_id').first().reset_index()\n",
    "drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9ddf85-2519-4487-8945-cf26b0a9b1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data loader for the dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class KBABygglovDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, drawings, df,scale = 1):\n",
    "        self.df = df\n",
    "        self.drawings = drawings\n",
    "        self.transforms = None\n",
    "        self.scale = scale\n",
    "        #self.task_ids = df[\"task_id\"].unique()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        task_id = self.drawings.iloc[idx][\"task_id\"]\n",
    "        rows = self.df[self.df[\"task_id\"] == task_id]\n",
    "        image = cv2.imread(rows.iloc[0][\"image\"])\n",
    "        #contour = find_bounding_box(image)\n",
    "        \n",
    "        \n",
    "        # Normalize bounding box coordinates\n",
    "        width, height = rows.iloc[0][\"original_width\"], rows.iloc[0][\"original_height\"]\n",
    "        target = {}\n",
    "        points = []\n",
    "        labels = []\n",
    "        boxes = [[0,0, image.shape[0], image.shape[1]]]\n",
    "\n",
    "        for i, row in rows.iterrows():\n",
    "            x1 = row['x'] * row[\"original_width\"] //100 \n",
    "            y1 = row['y'] * row[\"original_height\"] //100\n",
    "            points.append([x1, y1, 1])\n",
    "            labels.append(1)\n",
    "            if len(points) == 10:\n",
    "                break\n",
    "        while len(points) != 10:\n",
    "            points.append([0,0,0])\n",
    "        target[\"keypoints\"] = torch.as_tensor(points, \n",
    "                                dtype = torch.float32)\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, \n",
    "                                dtype = torch.float32)\n",
    "        target[\"labels\"]=torch.as_tensor([0],\n",
    "                        dtype = torch.int64)\n",
    "        target[\"image_id\"] = torch.as_tensor([task_id])\n",
    "        #scale the image and target to half size\n",
    "        image = cv2.resize(image, (image.shape[1]// self.scale,  image.shape[0]// self.scale))\n",
    "\n",
    "        target[\"boxes\"] = target[\"boxes\"]/ self.scale\n",
    "        target[\"keypoints\"] = target[\"keypoints\"]/ self.scale\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        #cropped_image, target = crop_to_contour(image, target, contour)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = KBABygglovDataset(drawings.iloc[:4* len(drawings)//5], df)\n",
    "test_dataset = KBABygglovDataset(drawings.iloc[4* len(drawings)//5:], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c437865-07c7-4de2-acd7-3298cbca4de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd5350-aa28-433a-87e1-ad8132954ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ffcf7-8aa9-478b-8fab-a4d9938d74ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    image, boxes = data\n",
    "    image.to(\"cpu\")\n",
    "    print(boxes)\n",
    "    #print(image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e199ecf9-6b5f-4324-a17b-3f321c07e649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collate image-target pairs into a tuple.\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "# Create the DataLoaders from the Datasets. \n",
    "train_dl = torch.utils.data.DataLoader(dataset, \n",
    "                                 batch_size = 4, \n",
    "                                 shuffle = True, \n",
    "                        collate_fn = collate_fn)\n",
    "'''val_dl = torch.utils.data.DataLoader(val_ds, \n",
    "                             batch_size = 4, \n",
    "                            shuffle = False, \n",
    "                    collate_fn = collate_fn)'''\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, \n",
    "                               batch_size = 1, \n",
    "                              shuffle = False, \n",
    "                      collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe326d-3ea8-4430-b4a0-f6669bde06d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=False,\n",
    "                                                                   num_keypoints=10,\n",
    "                                                                   num_classes = 1, # Background is the first class, object is the second class\n",
    "                                                              )\n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07bf448e-56f0-4663-8d55-66ddc9851b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unbatch(batch, device):\n",
    "    \"\"\"\n",
    "    Unbatches a batch of data from the Dataloader.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        X: list\n",
    "            List of images.\n",
    "        y: list\n",
    "            List of dictionaries.\n",
    "    \"\"\"\n",
    "    X, y = batch\n",
    "    X = [x.to(device) for x in X]\n",
    "    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
    "    return X, y\n",
    "def train_batch(batch, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Uses back propagation to train a model.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        model: torch model\n",
    "        optimizer: torch optimizer\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        loss: float\n",
    "            Sum of the batch losses.\n",
    "        losses: dict\n",
    "            Dictionary containing the individual losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    X, y = unbatch(batch, device = device)\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(X, y)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses\n",
    "@torch.no_grad()\n",
    "def validate_batch(batch, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    Evaluates a model's loss value using validation data.\n",
    "    Inputs\n",
    "        batch: tuple\n",
    "            Tuple containing a batch from the Dataloader.\n",
    "        model: torch model\n",
    "        optimizer: torch optimizer\n",
    "        device: str\n",
    "            Indicates which device (CPU/GPU) to use.\n",
    "    Returns\n",
    "        loss: float\n",
    "            Sum of the batch losses.\n",
    "        losses: dict\n",
    "            Dictionary containing the individual losses.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    X, y = unbatch(batch, device = device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    losses = model(X, y)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dc97c-5aed-4ccc-a844-0af5554bc675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torch_snippets import Report\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "# Assuming 'dataset' is an instance of 'KBABygglovDataset' and 'data_loader' is an instance of 'DataLoader'\n",
    "# Also assuming 'device' is defined (e.g., cuda or cpu)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                        lr = 0.005, \n",
    "                    momentum = 0.9, \n",
    "             weight_decay = 0.0005)\n",
    "\n",
    "\n",
    "log = Report(num_epochs)\n",
    "# FasterRCNN loss names.\n",
    "keys = [\"loss_classifier\", \n",
    "            \"loss_box_reg\", \n",
    "        \"loss_objectness\", \n",
    "        \"loss_rpn_box_reg\"]\n",
    "model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    N = len(train_dl)\n",
    "    for ix, batch in enumerate(train_dl):\n",
    "        loss, losses = train_batch(batch, model, \n",
    "                                optimizer, device)\n",
    "        # Record the current train loss.\n",
    "        pos = epoch + (ix + 1) / N\n",
    "        log.record(pos = pos, trn_loss = loss.item(), \n",
    "                    end = \"\\r\")\n",
    "    if test_dl is not None:\n",
    "        N = len(test_dl)\n",
    "        for ix, batch in enumerate(test_dl):\n",
    "            loss, losses = validate_batch(batch, model, \n",
    "                                        optimizer, device)\n",
    "            \n",
    "            # Record the current validation loss.\n",
    "            pos = epoch + (ix + 1) / N\n",
    "            log.record(pos = pos, val_loss = loss.item(), \n",
    "                        end = \"\\r\")\n",
    "log.report_avgs(epoch + 1)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c722dbe7-a134-4128-b54a-ea731c99fc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'marklinje.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047b2287-baea-4049-a77d-d0c8ec507f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load('marklinje.model',  map_location=torch.device('cpu')).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a13cdb46-0e99-4127-bf28-6b146a9a440a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_batch(batch, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X, _ = unbatch(batch, device = device)\n",
    "    predictions = model(X)\n",
    "    return [x.cpu() for x in X], predictions\n",
    "def predict(model, data_loader, device = \"cpu\"):\n",
    "    images = []\n",
    "    predictions = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(i)\n",
    "        X, p = predict_batch(batch, model, device)\n",
    "        images.append(X)\n",
    "        predictions.append(p)\n",
    "    \n",
    "    return images, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d152c31-de96-41bd-84f2-3e18ffb1aa11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c058a6c-231a-4ef2-9527-bc8fd0e3a14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images, predictions = predict(model, train_dl, device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22621975-5b86-4953-80e0-7b53c5d86a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8a7ba-d25a-4d0f-a4bf-52f991e00633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
